{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import joblib\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.25,>=1.21.1\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.45.0)\n",
      "Collecting python-slugify\n",
      "  Downloading python-slugify-4.0.0.tar.gz (8.8 kB)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (2.9)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72859 sha256=76e4fcdcdd555bdc4f3e231a8957999ef07313bb79ab8347e7c1ba8385b6655e\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/aa/e7/e7/eb3c3d514c33294d77ddd5a856bdd58dc9c1fabbed59a02a2b\n",
      "  Building wheel for python-slugify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-slugify: filename=python_slugify-4.0.0-py2.py3-none-any.whl size=5486 sha256=1ca2a96cf827de83b0d9e5f8d5f4d89789c864725dc8d0d2a83e1fb00a40cfc4\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/7c/26/30/5f3d95da00fe94d0c4a5ec5b4ffd2e1ae18545f5fa61752e52\n",
      "Successfully built kaggle python-slugify\n",
      "\u001b[31mERROR: fastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: kubernetes 10.1.0 has requirement pyyaml~=3.12, but you'll have pyyaml 5.3.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: urllib3, text-unidecode, python-slugify, kaggle\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "Successfully installed kaggle-1.5.6 python-slugify-4.0.0 text-unidecode-1.3 urllib3-1.24.3\n"
     ]
    }
   ],
   "source": [
    "# ! python -m pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p ~/.kaggle/\n",
    "# ! mv kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading bengaliai-cv19.zip to /home/jupyter/grapheme_classification\n",
      "100%|█████████████████████████████████████▉| 3.86G/3.88G [01:21<00:00, 66.8MB/s]\n",
      "100%|██████████████████████████████████████| 3.88G/3.88G [01:21<00:00, 50.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "#### Get image pickles #######\n",
    "#### Do this only once #######\n",
    "\n",
    "# ! kaggle competitions download -c bengaliai-cv19 -d input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! chmod 600 /home/jupyter/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  input/bengaliai-cv19.zip\n",
      "  inflating: input/class_map.csv     \n",
      "  inflating: input/class_map_corrected.csv  \n",
      "  inflating: input/sample_submission.csv  \n",
      "  inflating: input/test.csv          \n",
      "  inflating: input/test_image_data_0.parquet  \n",
      "  inflating: input/test_image_data_1.parquet  \n",
      "  inflating: input/test_image_data_2.parquet  \n",
      "  inflating: input/test_image_data_3.parquet  \n",
      "  inflating: input/train.csv         \n",
      "  inflating: input/train_image_data_0.parquet  \n",
      "  inflating: input/train_image_data_1.parquet  \n",
      "  inflating: input/train_image_data_2.parquet  \n",
      "  inflating: input/train_image_data_3.parquet  \n",
      "  inflating: input/train_multi_diacritics.csv  \n"
     ]
    }
   ],
   "source": [
    "# ! unzip input/bengaliai-cv19.zip -d input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train_0' 'Train_1' 'Train_2' ... 'Train_50207' 'Train_50208'\n",
      " 'Train_50209']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50210/50210 [00:25<00:00, 1954.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train_50210' 'Train_50211' 'Train_50212' ... 'Train_100417'\n",
      " 'Train_100418' 'Train_100419']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50210/50210 [00:29<00:00, 1689.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train_100420' 'Train_100421' 'Train_100422' ... 'Train_150627'\n",
      " 'Train_150628' 'Train_150629']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50210/50210 [00:27<00:00, 1827.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train_150630' 'Train_150631' 'Train_150632' ... 'Train_200837'\n",
      " 'Train_200838' 'Train_200839']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50210/50210 [00:28<00:00, 1786.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# for getting more speed while reading the images , we'll convert each image to pickle\n",
    "#### Do this only once ####\n",
    "files = glob.glob(\"input/train_*.parquet\")\n",
    "for f in files:\n",
    "    df = pd.read_parquet(f)\n",
    "    image_ids = df.image_id.values\n",
    "    df = df.drop(\"image_id\", axis = 1)\n",
    "    image_array = df.values\n",
    "    for j, img_id in tqdm(enumerate(image_ids), total = len(image_ids)):\n",
    "        joblib.dump(image_array[j,:], f\"input/image_pickles/{img_id}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
